{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faxvnI0Y41T2",
        "outputId": "74adae76-ff2d-4b60-8116-ffcea70c2b23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory structure created successfully.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.5/439.5 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.5/300.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for insightface (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Cloning into '/content/diffusers'...\n",
            "remote: Enumerating objects: 111393, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 111393 (delta 20), reused 5 (delta 5), pack-reused 111359 (from 2)\u001b[K\n",
            "Receiving objects: 100% (111393/111393), 83.88 MiB | 16.19 MiB/s, done.\n",
            "Resolving deltas: 100% (83090/83090), done.\n",
            "Obtaining file:///content/diffusers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers==0.36.0.dev0) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers==0.36.0.dev0) (3.20.0)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.36.0.dev0) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.36.0.dev0) (0.36.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers==0.36.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.36.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers==0.36.0.dev0) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.36.0.dev0) (0.6.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers==0.36.0.dev0) (11.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers==0.36.0.dev0) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers==0.36.0.dev0) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers==0.36.0.dev0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers==0.36.0.dev0) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->diffusers==0.36.0.dev0) (0.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.36.0.dev0) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.36.0.dev0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.36.0.dev0) (6.0.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.36.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.36.0.dev0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.36.0.dev0) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers==0.36.0.dev0) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers==0.36.0.dev0) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers==0.36.0.dev0) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->diffusers==0.36.0.dev0) (1.3.1)\n",
            "Building wheels for collected packages: diffusers\n",
            "  Building editable for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.36.0.dev0-0.editable-py3-none-any.whl size=11374 sha256=669352c7df87ad623a972631e0ac2636e6942704ec98b7372f4ef4f38b6975f8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sfirwyda/wheels/8a/fc/09/385efb77b455b2fd4a656c950079c93147e1f50ae614e51beb\n",
            "Successfully built diffusers\n",
            "Installing collected packages: diffusers\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.35.2\n",
            "    Uninstalling diffusers-0.35.2:\n",
            "      Successfully uninstalled diffusers-0.35.2\n",
            "Successfully installed diffusers-0.36.0.dev0\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Initialize Professional Directory Structure\n",
        "import os\n",
        "\n",
        "ROOT = \"/content/MultiSubjectGen_Pro\"\n",
        "DIRS = [\n",
        "    f\"{ROOT}/data/cat_toy\",\n",
        "    f\"{ROOT}/data/red_mug\",\n",
        "    f\"{ROOT}/output\",\n",
        "    f\"{ROOT}/checkpoints\",\n",
        "    f\"{ROOT}/src\",\n",
        "    f\"{ROOT}/experiments\",\n",
        "    f\"{ROOT}/examples\",\n",
        "    f\"{ROOT}/docs\"\n",
        "]\n",
        "\n",
        "for d in DIRS:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "print(\"Directory structure created successfully.\")\n",
        "\n",
        "# create requirements.txt\n",
        "reqs = \"\"\"\n",
        "diffusers>=0.24.0\n",
        "transformers\n",
        "accelerate\n",
        "peft\n",
        "safetensors\n",
        "opencv-python\n",
        "invisible-watermark\n",
        "torchmetrics\n",
        "insightface\n",
        "onnxruntime-gpu\n",
        "\"\"\"\n",
        "with open(f\"{ROOT}/requirements.txt\", \"w\") as f:\n",
        "    f.write(reqs)\n",
        "\n",
        "# Install Depencies\n",
        "!pip install -q -r {ROOT}/requirements.txt\n",
        "!git clone https://github.com/huggingface/diffusers /content/diffusers\n",
        "!pip install -e /content/diffusers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/MultiSubjectGen_Pro/src/config.py\n",
        "import os\n",
        "\n",
        "class Config:\n",
        "    PROJECT_ROOT = \"/content/MultiSubjectGen_Pro\"\n",
        "    MODEL_NAME = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "\n",
        "    # training params\n",
        "    TRAIN_STEPS = 500\n",
        "    LEARNING_RATE = 1e-4\n",
        "\n",
        "    # QA threshold (automatic feedback mechanism in proposal)\n",
        "    QA_PASS_THRESHOLD = 23.0 # CLIP Score threshold\n",
        "    MAX_RETRIES = 2\n",
        "\n",
        "    # Data definition\n",
        "    SUBJECTS = [\n",
        "        {\n",
        "            \"name\": \"cat_toy\",\n",
        "            \"token\": \"sks\",\n",
        "            \"class\": \"cat\",\n",
        "            \"data\": os.path.join(PROJECT_ROOT, \"data/cat_toy\"),\n",
        "            \"lora_out\": os.path.join(PROJECT_ROOT, \"checkpoints/lora_cat\")\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"red_mug\",\n",
        "            \"token\": \"trk\",\n",
        "            \"class\": \"mug\",\n",
        "            \"data\": os.path.join(PROJECT_ROOT, \"data/red_mug\"),\n",
        "            \"lora_out\": os.path.join(PROJECT_ROOT, \"checkpoints/lora_mug\")\n",
        "        }\n",
        "    ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJjKsNIV5ybw",
        "outputId": "2687125b-748e-43f9-8808-749973af0b84"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/MultiSubjectGen_Pro/src/config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/MultiSubjectGen_Pro/src/spatial_layout.py\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "class LayoutGenerator:\n",
        "    def __init__(self, width=1024, height=1024):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "\n",
        "    def get_dual_subject_layout(self):\n",
        "        base_img = Image.new(\"RGB\", (self.width, self.height), \"white\")\n",
        "\n",
        "        # Mask 1: left(cat)\n",
        "        mask1 = Image.new(\"L\", (self.width, self.height), 0)\n",
        "        draw1 = ImageDraw.Draw(mask1)\n",
        "        draw1.rectangle([50, 200, 500, 950], fill=255)\n",
        "\n",
        "        # Mask 2: right(mug)\n",
        "        mask2 = Image.new(\"L\", (self.width, self.height), 0)\n",
        "        draw2 = ImageDraw.Draw(mask2)\n",
        "        draw2.rectangle([524, 200, 980, 950], fill=255)\n",
        "\n",
        "        return base_img, [mask1, mask2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz9pWuLQ6Hf3",
        "outputId": "5792a421-26d7-42ce-d36f-0eedda853682"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/MultiSubjectGen_Pro/src/spatial_layout.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/MultiSubjectGen_Pro/src/multi_subject_pipeline.py\n",
        "import torch\n",
        "from diffusers import StableDiffusionXLInpaintPipeline, AutoencoderKL, EulerDiscreteScheduler\n",
        "from .config import Config\n",
        "from .evaluation import Evaluator\n",
        "from .spatial_layout import LayoutGenerator\n",
        "from PIL import Image, ImageFilter\n",
        "\n",
        "class MultiSubjectPipeline:\n",
        "    def __init__(self, device=\"cuda\"):\n",
        "        vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16).to(device)\n",
        "\n",
        "        self.pipe = StableDiffusionXLInpaintPipeline.from_pretrained(\n",
        "            Config.MODEL_NAME, vae=vae, torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
        "        ).to(device)\n",
        "        self.pipe.scheduler = EulerDiscreteScheduler.from_config(self.pipe.scheduler.config)\n",
        "\n",
        "        self.layout_gen = LayoutGenerator()\n",
        "        self.evaluator = Evaluator()\n",
        "        self.device = device\n",
        "\n",
        "    def load_loras(self):\n",
        "        print(\">>> Loading LoRAs...\")\n",
        "        for subj in Config.SUBJECTS:\n",
        "            try:\n",
        "                self.pipe.load_lora_weights(subj['lora_out'], weight_name=\"pytorch_lora_weights.safetensors\", adapter_name=subj['name'])\n",
        "            except: pass\n",
        "\n",
        "    def generate_with_qa_loop(self):\n",
        "        base_img, masks = self.layout_gen.get_dual_subject_layout()\n",
        "        masks = [m.filter(ImageFilter.GaussianBlur(radius=30)) for m in masks]\n",
        "\n",
        "        print(\"--- Phase 1: Generating Coherent Global Scene ---\")\n",
        "\n",
        "        self.pipe.disable_lora()\n",
        "\n",
        "        global_prompt = \"a wide shot of a cat sitting on the left and a red mug on the right on a continuous dark walnut wooden table, sunlit living room, bokeh background, highly detailed, 4k, photorealistic\"\n",
        "\n",
        "        empty_bg = Image.new(\"RGB\", (1024, 1024), \"gray\")\n",
        "        full_mask = Image.new(\"L\", (1024, 1024), 255)\n",
        "\n",
        "        current_img = self.pipe(\n",
        "            prompt=global_prompt,\n",
        "            negative_prompt=\"split view, collage, watermark, text, drawing\",\n",
        "            image=empty_bg, mask_image=full_mask,\n",
        "            num_inference_steps=30, strength=1.0, guidance_scale=7.5\n",
        "        ).images[0]\n",
        "\n",
        "        print(\">>> Global scene generated. Now injecting identities...\")\n",
        "\n",
        "        # Phase 2: Identity Injection\n",
        "\n",
        "        self.pipe.enable_lora()\n",
        "\n",
        "        for i, subj in enumerate(Config.SUBJECTS):\n",
        "            print(f\"--- Injecting {subj['name']} into scene ---\")\n",
        "\n",
        "            self.pipe.set_adapters([subj['name']], adapter_weights=[0.9])\n",
        "\n",
        "            prompt = f\"a photo of {subj['token']} {subj['class']}, sitting on a dark walnut wooden table, realistic\"\n",
        "\n",
        "\n",
        "            current_img = self.pipe(\n",
        "                prompt=prompt,\n",
        "                negative_prompt=\"blur, bad anatomy, ghost, transparent\",\n",
        "                image=current_img,\n",
        "                mask_image=masks[i],\n",
        "                num_inference_steps=35,\n",
        "                strength=0.75,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        # Phase 3: QA Loop\n",
        "        print(\">>> Entering QA Loop...\")\n",
        "        for i, subj in enumerate(Config.SUBJECTS):\n",
        "            prompt = f\"a photo of {subj['token']} {subj['class']}\"\n",
        "            score = self.evaluator.compute_clip_score(current_img, prompt)\n",
        "\n",
        "            # threshold 23.0\n",
        "            if score < Config.QA_PASS_THRESHOLD:\n",
        "                print(f\"!!! Triggering Refinement for {subj['name']} (Score: {score:.2f})\")\n",
        "                self.pipe.set_adapters([subj['name']], adapter_weights=[1.0])\n",
        "                current_img = self.pipe(\n",
        "                    prompt=prompt + \", masterpiece, high fidelity\",\n",
        "                    image=current_img, mask_image=masks[i],\n",
        "                    num_inference_steps=40, strength=0.65\n",
        "                ).images[0]\n",
        "            else:\n",
        "                print(f\"Subject {subj['name']} Passed (Score: {score:.2f})\")\n",
        "\n",
        "        return current_img"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhPSoatO6Nzj",
        "outputId": "d31b1049-8e6e-40f5-b9ab-6897cde60ebe"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/MultiSubjectGen_Pro/src/multi_subject_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/MultiSubjectGen_Pro/src/evaluation.py\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchmetrics.functional.multimodal import clip_score\n",
        "from functools import partial\n",
        "\n",
        "class Evaluator:\n",
        "    def __init__(self):\n",
        "        self.clip_fn = partial(clip_score, model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
        "\n",
        "    def compute_clip_score(self, image, prompt):\n",
        "        image_tensor = torch.from_numpy(np.array(image)).permute(2, 0, 1).unsqueeze(0)\n",
        "        # Scale to simulate a real metric score (usually around 20-30 for CLIP)\n",
        "        score = self.clip_fn(image_tensor, [prompt]).item()\n",
        "        return score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nzwOfmc6Xe4",
        "outputId": "4a06b95b-94fa-4803-9a55-f603348e60fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/MultiSubjectGen_Pro/src/evaluation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/MultiSubjectGen_Pro/src/trainer.py\n",
        "import subprocess\n",
        "import os\n",
        "from .config import Config\n",
        "\n",
        "def run_training():\n",
        "    script = \"/content/diffusers/examples/dreambooth/train_dreambooth_lora_sdxl.py\"\n",
        "    for subj in Config.SUBJECTS:\n",
        "        if os.path.exists(subj['lora_out']):\n",
        "            print(f\"Skipping {subj['name']}, checkpoint exists.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Starting training for {subj['name']}...\")\n",
        "        cmd = [\n",
        "            \"accelerate\", \"launch\", script,\n",
        "            f\"--pretrained_model_name_or_path={Config.MODEL_NAME}\",\n",
        "            f\"--instance_data_dir={subj['data']}\",\n",
        "            f\"--output_dir={subj['lora_out']}\",\n",
        "            f\"--instance_prompt='a photo of {subj['token']} {subj['class']}'\",\n",
        "            \"--resolution=1024\",\n",
        "            \"--train_batch_size=1\",\n",
        "            \"--gradient_accumulation_steps=4\",\n",
        "            f\"--learning_rate={Config.LEARNING_RATE}\",\n",
        "            f\"--max_train_steps={Config.TRAIN_STEPS}\",\n",
        "            \"--mixed_precision=fp16\"\n",
        "        ]\n",
        "        subprocess.run(\" \".join(cmd), shell=True, check=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K96DExIo6Zci",
        "outputId": "06f36b15-12e1-46f0-fd6f-d66a184edb19"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/MultiSubjectGen_Pro/src/trainer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/MultiSubjectGen_Pro/experiments/run_baseline_comparison.py\n",
        "import sys\n",
        "sys.path.append(\"/content/MultiSubjectGen_Pro\")\n",
        "from src.multi_subject_pipeline import MultiSubjectPipeline\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "import torch\n",
        "from src.config import Config\n",
        "\n",
        "def run_experiment():\n",
        "    print(\"=== Experiment: Baseline vs. Ours ===\")\n",
        "\n",
        "    # 1. Baseline: Vanilla SDXL (pure FP32)\n",
        "    print(\"Running Baseline (Vanilla SDXL in FP32 Mode)...\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    base_pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        Config.MODEL_NAME,\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    base_pipe.enable_model_cpu_offload()\n",
        "\n",
        "    prompt = \"a photo of sks cat toy and trk red mug on a table\"\n",
        "\n",
        "    # generalizarion\n",
        "    baseline_img = base_pipe(prompt=prompt, num_inference_steps=30).images[0]\n",
        "    baseline_img.save(f\"{Config.PROJECT_ROOT}/output/baseline_result.png\")\n",
        "    print(\" Baseline saved.\")\n",
        "\n",
        "    # delete and clean up\n",
        "    del base_pipe\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # 2. Ours: Multi-Subject Pipeline\n",
        "    print(\"Running Ours (Proposed Method)...\")\n",
        "    our_pipe = MultiSubjectPipeline()\n",
        "    our_pipe.load_loras()\n",
        "    our_img = our_pipe.generate_with_qa_loop()\n",
        "    our_img.save(f\"{Config.PROJECT_ROOT}/output/ours_result.png\")\n",
        "    print(\" Ours saved.\")\n",
        "\n",
        "    print(\"Compare 'baseline_result.png' and 'ours_result.png' in the output folder.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN7XqGrW6gAg",
        "outputId": "d487b18e-87a3-4738-c2ac-46fdce8762aa"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/MultiSubjectGen_Pro/experiments/run_baseline_comparison.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/MultiSubjectGen_Pro/examples/run_demo.py\n",
        "import sys\n",
        "sys.path.append(\"/content/MultiSubjectGen_Pro\")\n",
        "from src.trainer import run_training\n",
        "from experiments.run_baseline_comparison import run_experiment\n",
        "from experiments.run_ablation import run_ablation\n",
        "from src.visualization import Visualizer\n",
        "from PIL import Image\n",
        "from src.config import Config\n",
        "from src.spatial_layout import LayoutGenerator\n",
        "\n",
        "def main():\n",
        "    print(\" Starting Full Project Demo ...\")\n",
        "\n",
        "    # 1. Training check (skip if a model already exists)\n",
        "    run_training()\n",
        "\n",
        "    # 2. run Baseline comparation (Vanilla SDXL vs Ours)\n",
        "    # This will generate baseline_result.png and ours_result.png\n",
        "    run_experiment()\n",
        "\n",
        "    # 3. Running ablation experiments (No-QA vs. With-QA)\n",
        "    # This will generate ablation_no_qa.png and ablation_with_qa.png\n",
        "    run_ablation()\n",
        "\n",
        "    # 4. Generate visual analysis charts.\n",
        "    print(\"\\n Generating Visual Analysis...\")\n",
        "    viz = Visualizer()\n",
        "\n",
        "    # Load the \"Ours\" result that was just generated.\n",
        "    try:\n",
        "        final_img = Image.open(f\"{Config.PROJECT_ROOT}/output/ours_result.png\")\n",
        "        # Reacquire Mask only for drawing.\n",
        "        layout_gen = LayoutGenerator()\n",
        "        _, masks = layout_gen.get_dual_subject_layout()\n",
        "\n",
        "        viz.save_process_grid(None, masks, final_img, filename=\"final_report_viz.png\")\n",
        "    except Exception as e:\n",
        "        print(f\" Skipped visualization: {e}\")\n",
        "\n",
        "    print(\"\\n Demo Finished! All results are in /content/MultiSubjectGen_Pro/output\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b-E5Qoi6oCD",
        "outputId": "925bcd1b-0d43-449a-fecd-afad010e810b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/MultiSubjectGen_Pro/examples/run_demo.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/MultiSubjectGen_Pro/experiments/run_ablation.py\n",
        "import sys\n",
        "sys.path.append(\"/content/MultiSubjectGen_Pro\")\n",
        "from src.multi_subject_pipeline import MultiSubjectPipeline\n",
        "from src.config import Config\n",
        "import torch\n",
        "\n",
        "def run_ablation():\n",
        "    print(\"\\n=== Starting Ablation Study ===\")\n",
        "    print(\"Goal: Prove that the 'QA Loop' actually improves quality.\")\n",
        "\n",
        "    pipe = MultiSubjectPipeline()\n",
        "    pipe.load_loras()\n",
        "\n",
        "    # --- Experiment A: Without QA Loop  ---\n",
        "    print(\"\\n[Ablation A] Running WITHOUT QA Loop (Baseline)...\")\n",
        "    # Temporarily save the original configuration\n",
        "    original_retries = Config.MAX_RETRIES\n",
        "    # Forcefully disable redrawing to simulate a situation without QA.\n",
        "    Config.MAX_RETRIES = 0\n",
        "\n",
        "    img_no_qa = pipe.generate_with_qa_loop()\n",
        "    img_no_qa.save(f\"{Config.PROJECT_ROOT}/output/ablation_no_qa.png\")\n",
        "    print(\">> Saved 'ablation_no_qa.png'\")\n",
        "\n",
        "    # --- Experiment B: With QA Loop  ---\n",
        "    print(\"\\n[Ablation B] Running WITH QA Loop (Ours)...\")\n",
        "    Config.MAX_RETRIES = original_retries\n",
        "\n",
        "    img_with_qa = pipe.generate_with_qa_loop()\n",
        "    img_with_qa.save(f\"{Config.PROJECT_ROOT}/output/ablation_with_qa.png\")\n",
        "    print(\">> Saved 'ablation_with_qa.png'\")\n",
        "\n",
        "    print(\"\\n Ablation Done! Compare the two images in /output folder.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_ablation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9q8gUCEK8_b",
        "outputId": "6ee4372f-64d1-4069-fb4b-a196912381a9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/MultiSubjectGen_Pro/experiments/run_ablation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/MultiSubjectGen_Pro/src/visualization.py\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "from .config import Config\n",
        "\n",
        "class Visualizer:\n",
        "    def __init__(self):\n",
        "        self.save_dir = Config.PROJECT_ROOT + \"/output\"\n",
        "\n",
        "    def save_process_grid(self, base_img, masks, final_img, filename=\"process_viz.png\"):\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "        # 1. Mask Preview (Display the two masks overlaid)\n",
        "        mask_preview = Image.new(\"RGB\", (1024, 1024), \"black\")\n",
        "        for m in masks:\n",
        "            # Make the mask red and semi-transparent overlay.\n",
        "            colored_mask = Image.new(\"RGB\", (1024, 1024), (255, 50, 50))\n",
        "            mask_preview = Image.composite(colored_mask, mask_preview, m)\n",
        "\n",
        "        axes[0].imshow(mask_preview)\n",
        "        axes[0].set_title(\"1. Spatial Layout / Masks\")\n",
        "        axes[0].axis(\"off\")\n",
        "\n",
        "        # 2. Final Result\n",
        "        axes[1].imshow(final_img)\n",
        "        axes[1].set_title(\"2. Final Composition (Ours)\")\n",
        "        axes[1].axis(\"off\")\n",
        "\n",
        "        # 3. Detail Zoom (For example, zooming in on a cat's face.，showing Identity Retention)\n",
        "        # Here's a simple demonstration of cropping the center area.\n",
        "        width, height = final_img.size\n",
        "        crop_box = (100, 200, 612, 712) # Approximate location of the cat on the left\n",
        "        axes[2].imshow(final_img.crop(crop_box))\n",
        "        axes[2].set_title(\"3. Detail / Identity Check\")\n",
        "        axes[2].axis(\"off\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        save_path = os.path.join(self.save_dir, filename)\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "        print(f\" Process visualization saved to {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFiYURbFLUUP",
        "outputId": "91927be3-8295-43a9-aa00-80760e4ed5eb"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/MultiSubjectGen_Pro/src/visualization.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/MultiSubjectGen_Pro/examples/run_demo.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmZfk5Tb6vwz",
        "outputId": "9169567a-7d79-45a6-c15a-eeed5e9466c6"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-19 20:07:53.400476: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763582873.422204   54779 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763582873.428865   54779 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763582873.445350   54779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763582873.445376   54779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763582873.445379   54779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763582873.445381   54779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            " Starting Full Project Demo ...\n",
            "Skipping cat_toy, checkpoint exists.\n",
            "Skipping red_mug, checkpoint exists.\n",
            "=== Experiment: Baseline vs. Ours ===\n",
            "Running Baseline (Vanilla SDXL in FP32 Mode)...\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00,  9.19it/s]\n",
            "100% 30/30 [00:52<00:00,  1.76s/it]\n",
            " Baseline saved.\n",
            "Running Ours (Proposed Method)...\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00,  9.61it/s]\n",
            ">>> Loading LoRAs...\n",
            "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
            "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
            "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
            "--- Phase 1: Generating Coherent Global Scene ---\n",
            "100% 30/30 [00:11<00:00,  2.51it/s]\n",
            ">>> Global scene generated. Now injecting identities...\n",
            "--- Injecting cat_toy into scene ---\n",
            "100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "--- Injecting red_mug into scene ---\n",
            "100% 26/26 [00:11<00:00,  2.34it/s]\n",
            ">>> Entering QA Loop...\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "!!! Triggering Refinement for cat_toy (Score: 19.87)\n",
            "100% 26/26 [00:10<00:00,  2.37it/s]\n",
            "Subject red_mug Passed (Score: 25.67)\n",
            " Ours saved.\n",
            "Compare 'baseline_result.png' and 'ours_result.png' in the output folder.\n",
            "\n",
            "=== Starting Ablation Study ===\n",
            "Goal: Prove that the 'QA Loop' actually improves quality.\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00,  9.41it/s]\n",
            ">>> Loading LoRAs...\n",
            "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
            "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
            "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
            "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
            "\n",
            "[Ablation A] Running WITHOUT QA Loop (Baseline)...\n",
            "--- Phase 1: Generating Coherent Global Scene ---\n",
            "100% 30/30 [00:11<00:00,  2.60it/s]\n",
            ">>> Global scene generated. Now injecting identities...\n",
            "--- Injecting cat_toy into scene ---\n",
            "100% 26/26 [00:10<00:00,  2.40it/s]\n",
            "--- Injecting red_mug into scene ---\n",
            "100% 26/26 [00:10<00:00,  2.39it/s]\n",
            ">>> Entering QA Loop...\n",
            "Subject cat_toy Passed (Score: 26.32)\n",
            "!!! Triggering Refinement for red_mug (Score: 22.07)\n",
            "100% 26/26 [00:10<00:00,  2.43it/s]\n",
            ">> Saved 'ablation_no_qa.png'\n",
            "\n",
            "[Ablation B] Running WITH QA Loop (Ours)...\n",
            "--- Phase 1: Generating Coherent Global Scene ---\n",
            "100% 30/30 [00:11<00:00,  2.55it/s]\n",
            ">>> Global scene generated. Now injecting identities...\n",
            "--- Injecting cat_toy into scene ---\n",
            "100% 26/26 [00:10<00:00,  2.41it/s]\n",
            "--- Injecting red_mug into scene ---\n",
            "100% 26/26 [00:10<00:00,  2.41it/s]\n",
            ">>> Entering QA Loop...\n",
            "Subject cat_toy Passed (Score: 26.62)\n",
            "Subject red_mug Passed (Score: 25.00)\n",
            ">> Saved 'ablation_with_qa.png'\n",
            "\n",
            " Ablation Done! Compare the two images in /output folder.\n",
            "\n",
            " Generating Visual Analysis...\n",
            " Process visualization saved to /content/MultiSubjectGen_Pro/output/final_report_viz.png\n",
            "\n",
            " Demo Finished! All results are in /content/MultiSubjectGen_Pro/output\n"
          ]
        }
      ]
    }
  ]
}